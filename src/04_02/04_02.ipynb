{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62b685c1-fe44-4f01-b9e3-5808cf10752f",
   "metadata": {},
   "source": [
    "# Chat With Your Data\n",
    "\n",
    "## Preserve conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bed94b7-d5da-4c5b-a812-25ecbb1601a6",
   "metadata": {},
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603889ff-e355-4c9e-929a-45e8150aa0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759ae117-0aa2-4c09-b5bd-7ce1e106ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd511c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca531938",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1839e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c527ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9fcee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchainhub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef23fe7-5c16-4f30-97e4-b679c75e716d",
   "metadata": {},
   "source": [
    "## Load OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77f69144-98d1-46a1-94be-8eea00065278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "OPENAI_API_KEY=os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959501e7-3a7c-456e-93f1-0561979e81c4",
   "metadata": {},
   "source": [
    "## Prompt model with no knowledge of the Voynich manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71c0d74-1911-4082-8bc2-c930607fb800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "#initialize the LLM we'll use - OpenAI GPT 3.5 Turbo\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67a63a96-ddfb-4beb-a699-d2c035ae67cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The Voynich manuscript is a mysterious document written in an unknown script and language, with detailed illustrations of plants, astrological symbols, and human figures. Some researchers have speculated that the manuscript may contain medicinal insights or herbal remedies based on the plant illustrations and the overall layout of the text.\\n\\nHowever, due to the enigmatic nature of the manuscript and the inability to decipher the text, it is currently impossible to definitively determine what, if any, medicinal insights are contained within its pages. Some theories suggest that the manuscript may contain information on alchemical practices, herbal medicine, or even early forms of pharmacology.\\n\\nWithout a clear understanding of the language and symbols used in the manuscript, it is difficult to extract any concrete medicinal insights. The Voynich manuscript continues to be a source of fascination and intrigue for researchers, but its true meaning and purpose remain a mystery.', response_metadata={'token_usage': {'completion_tokens': 174, 'prompt_tokens': 19, 'total_tokens': 193}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6adce9e8-ec7b-43fe-8bb4-0d925ef55c39-0', usage_metadata={'input_tokens': 19, 'output_tokens': 174, 'total_tokens': 193})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prompt the model with no additional knowledge of the Voynich manuscript beyond pretraining \n",
    "llm.invoke(\"What are the medicinal insights from the Voynich manuscript?\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "135b5589-664b-4ce7-aa2f-d593e9758c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Aetherfloris Ventus is a Latin term that translates to \"air flower wind.\" It is a concept that refers to the combination of air, flowers, and wind in a harmonious and interconnected way, symbolizing the beauty and interconnectedness of nature.', response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 16, 'total_tokens': 68}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f18b0515-4360-42df-8877-2fe8eeb545f7-0', usage_metadata={'input_tokens': 16, 'output_tokens': 52, 'total_tokens': 68})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is Aetherfloris Ventus?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafecdae-eaf9-471f-b69d-9b1c3ac0b9d2",
   "metadata": {},
   "source": [
    "## Load vector database from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0788aef1-0fe8-4205-ba3d-741bb5cf392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "db = FAISS.load_local(\"../faiss_index\", \n",
    "                      OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\"), \n",
    "                      allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b7e3f2-eaba-4fa1-aced-d79431959cd9",
   "metadata": {},
   "source": [
    "## Configure retriever\n",
    "### Use the similarity search capabilities of a vector store to facilitate retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ec80ca7-918a-4898-b64d-4cee56d8141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4dfe35-f043-45c1-a244-0e4505807af4",
   "metadata": {},
   "source": [
    "## Implement a chain\n",
    "### Chain together multiple calls in a logical sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3390bcda-1b75-4eaf-96b6-73c583c08760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbe8886a-c809-428c-b2ff-b8d4c1b933f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89b37521-0732-49cd-996c-5a955d64df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#combine multiple steps in a single chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser() #convert the chat message to a string\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd0fdc-8a89-42f5-9588-c728c99aaafa",
   "metadata": {},
   "source": [
    "## Send LLM's response to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33d8b346-435b-4d50-a9b3-729e90bde815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Voynich manuscript contains detailed anatomical diagrams of mythical beings, possibly used for medicinal or alchemical purposes, providing insights into ancient medical knowledge intertwined with fantasy. The manuscript also includes depictions of mysterious herbs with unique properties, possibly indicating their medicinal uses in preparing potions or remedies. Additionally, the manuscript features colorful illustrations of dishes prepared from unknown or exotic ingredients, likely used for special feasts or medicinal purposes, reflecting a cultural crossover between culinary practices and medicinal knowledge."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"What are the medicinal insights from the Voynich manuscript?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6667483b-9d8d-46f7-bbf9-7417249da5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aetherfloris Ventus is a delicate plant of ethereal beauty with petals lighter than air, appearing to float freely, untethered by gravity. It is believed to have originated from the breath of the winds and nurtured by the whispers of the clouds. The plant's essence, captured in rare vials, is said to bestow the gift of lightness and uplift the spirits."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"What is Aetherfloris Ventus?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93d4f8e0-14cb-40d0-b710-542d27c31bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most important part of the Voynich manuscript is the detailed anatomical diagrams of mythical beings, possibly used for medicinal or alchemical purposes, with annotations explaining the function of each organ and system. These illustrations offer a unique blend of fantasy and precise anatomical accuracy, providing insights into ancient medical knowledge intertwined with fantasy. The annotated diagrams may have served as a practical guide for medicinal and alchemical uses, possibly indicating the functions of various organs and their importance in ancient practices."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"What's the most important part of the Voynich manuscript?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc0fd64-aa2e-46fe-a2c6-a0b8ca109d76",
   "metadata": {},
   "source": [
    "## Preserve Conversation History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "732ea316-19cb-435f-b822-fb0647925980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "system_prompt = \"\"\"Given the chat history and a recent user question \\\n",
    "generate a new standalone question \\\n",
    "that can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed or otherwise return it as is.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retriever_with_history = create_history_aware_retriever(\n",
    "    llm, retriever, prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6593d472-89cc-4b7b-b61a-dad74e864d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Given the chat history and a recent user question generate a new standalone question that can be understood without the chat history. Do NOT answer the question, just reformulate it if needed or otherwise return it as is.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e56844fc-4d86-4cfb-85c9-1ca55794646f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chat_history', 'input']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77d1cc27-1dd6-4210-8eab-c44e1802afd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aecbdb5a-a390-4e4e-927a-501b6e4b232e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Given the chat history and a recent user question generate a new standalone question that can be understood without the chat history. Do NOT answer the question, just reformulate it if needed or otherwise return it as is.')),\n",
       " MessagesPlaceholder(variable_name='chat_history'),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "682e7231-0714-4533-8410-9a11306b7506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "| VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7c3d9baf30d0>, search_kwargs={'k': 6}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Given the chat history and a recent user question generate a new standalone question that can be understood without the chat history. Do NOT answer the question, just reformulate it if needed or otherwise return it as is.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7c3dc0205480>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7c3dc0206b90>, model_name='gpt-3.5-turbo-0125', openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "| StrOutputParser()\n",
       "| VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7c3d9baf30d0>, search_kwargs={'k': 6})), config={'run_name': 'chat_retriever_chain'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_with_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
